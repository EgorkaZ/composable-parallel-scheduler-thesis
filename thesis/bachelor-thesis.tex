\documentclass[times,specification,annotation]{itmo-student-thesis}

%% Опции пакета:
%% - specification - если есть, генерируется задание, иначе не генерируется
%% - annotation - если есть, генерируется аннотация, иначе не генерируется
%% - times - делает все шрифтом Times New Roman, собирается с помощью xelatex
%% - languages={...} - устанавливает перечень используемых языков. По умолчанию это {english,russian}.
%%                     Последний из языков определяет текст основного документа.

%% Делает запятую в формулах более интеллектуальной, например:
%% $1,5x$ будет читаться как полтора икса, а не один запятая пять иксов.
%% Однако если написать $1, 5x$, то все будет как прежде.
\usepackage{icomma}

\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}

\usepackage{svg}

%% Один из пакетов, позволяющий делать таблицы на всю ширину текста.
\usepackage{tabularx}

%% Данные пакеты необязательны к использованию в бакалаврских/магистерских
%% Они нужны для иллюстративных целей
%% Начало
\usepackage{tikz}
\usetikzlibrary{arrows}

%% Указываем файл с библиографией.
\addbibresource{bachelor-thesis.bib}

\begin{document}

\studygroup{M34391}
\title{Разработка алгоритма быстрого распределения работы для компонуемых планировщиков задач}
\author{Воркожоков Денис Вадимович}{Воркожоков Д.В.}
\supervisor{Аксенов Виталий Евгеньевич}{Аксенов В.Е.}{доцент.}{}
\publishyear{2023}
%% Дата выдачи задания. Можно не указывать, тогда надо будет заполнить от руки.
\startdate{01}{апреля}{2023}
%% Срок сдачи студентом работы. Можно не указывать, тогда надо будет заполнить от руки.
\finishdate{31}{мая}{2023}
%% Дата защиты. Можно не указывать, тогда надо будет заполнить от руки.
\defencedate{15}{июня}{2023}

\addconsultant{Малахов Антон Александрович}{Малахов А. А.}

\secretary{Штумпф С.А.}

%% Задание
%%% Техническое задание и исходные данные к работе
% \technicalspec{Требуется разработать стилевой файл для системы \LaTeX, позволяющий оформлять бакалаврские работы и магистерские диссертации
% на кафедре компьютерных технологий Университета ИТМО. Стилевой файл должен генерировать титульную страницу пояснительной записки,
% задание, аннотацию и содержательную часть пояснительной записк. Первые три документа должны максимально близко соответствовать шаблонам документов,
% принятым в настоящий момент на кафедре, в то время как содержательная часть должна максимально близко соответствовать ГОСТ~7.0.11-2011
% на диссертацию.}

%%% Содержание выпускной квалификационной работы (перечень подлежащих разработке вопросов)
% \plannedcontents{Пояснительная записка должна демонстрировать использование наиболее типичных конструкций, возникающих при составлении
% пояснительной записки (перечисления, рисунки, таблицы, листинги, псевдокод), при этом должна быть составлена так, что демонстрируется
% корректность работы стилевого файла. В частности, записка должна содержать не менее двух приложений (для демонстрации нумерации рисунков и таблиц
% по приложениям согласно ГОСТ) и не менее десяти элементов нумерованного перечисления первого уровня вложенности (для демонстрации корректности
% используемого при нумерации набора русских букв).}

%%% Исходные материалы и пособия 
% \plannedsources{\begin{enumerate}
%     \item ГОСТ~7.0.11-2011 <<Диссертация и автореферат диссертации>>;
%     \item С.М. Львовский. Набор и верстка в системе \LaTeX;
%     \item предыдущий комплект стилевых файлов, использовавшийся на кафедре компьютерных технологий.
% \end{enumerate}}

%%% Цель исследования
% \researchaim{Разработка удобного стилевого файла \LaTeX
%              для бакалавров и магистров кафедры компьютерных технологий.}

%%% Задачи, решаемые в ВКР
% \researchtargets{\begin{enumerate}
%     \item обеспечение соответствия титульной страницы, задания и аннотации шаблонам, принятым в настоящее время на кафедре;
%     \item обеспечение соответствия содержательной части пояснительной записки требованиям ГОСТ~7.0.11-2011 <<Диссертация и автореферат диссертации>>;
%     \item обеспечение относительного удобства в использовании~--- указание данных об авторе и научном руководителе один раз и в одном месте, автоматический подсчет числа тех или иных источников.
% \end{enumerate}}

%%% Использование современных пакетов компьютерных программ и технологий
% \addadvancedsoftware{Пакет \texttt{tabularx} для чуть более продвинутых таблиц}{\ref{sec:tables}, Приложения~\ref{sec:app:1}, \ref{sec:app:2}}
% \addadvancedsoftware{Пакет \texttt{biblatex} и программное средство \texttt{biber}}{Список использованных источников}

%%% Краткая характеристика полученных результатов 
% \researchsummary{Получился, надо сказать, практически неплохой стилевик. В 2015--2018 годах
% его уже использовали некоторые бакалавры и магистры. Надеюсь на продолжение.}

%%% Гранты, полученные при выполнении работы 
% \researchfunding{Автор разрабатывал этот стилевик исключительно за свой счет и на
% добровольных началах. Однако значительная его часть была бы невозможна, если бы
% автор не написал в свое время кандидатскую диссертацию в \LaTeX,
% а также не отвечал за формирование кучи научно-технических отчетов по гранту,
% известному как <<5-в-100>>, что происходило при государственной финансовой поддержке
% ведущих университетов Российской Федерации (субсидия 074-U01).}

%%% Наличие публикаций и выступлений на конференциях по теме выпускной работы
% \researchpublications{По теме этой работы я (к счастью!) ничего не публиковал.
% \begin{refsection}
% Однако покажу, как можно ссылаться на свои публикации из списка литературы:
% \nocite{example-english, example-russian}
% \printannobibliography
% \end{refsection}
% }

%% Эта команда генерирует титульный лист и аннотацию.
\maketitle{Бакалавр}

%% Оглавление
\tableofcontents

%% Макрос для введения. Совместим со старым стилевиком.
\startprefacepage
С ростом объема данных возникает необходимость в их быстрой обработке. В прошлом производительность одного ядра процессоров росла экспоненциально, однако к началу 2000-х годов этот рост замедлился и индустрии пришлось искать другие способы увеличивать производительность. Один из таких способов -- горизонтальное масштабирование, которое достигается за счет использования параллельных алгоритмов, использующих несколько ядер или даже процессоров одновременно. Такие алгоритмы позволяют значительно увеличить скорость обработки данных, в лучшем случае -- повысить её кратно количеству ядер процессоров.

Для достижения максимального ускорения алгоритма, необходимо эффективно распределять работу между ядрами процессоров, избегая простоя ресурсов. Например, для простейшего алгоритма \texttt{parallel\_for} необходимо учитывать, что различные его итерации могут занимать разное время и нагрузка не всегда будет равномерно распределена, поэтому подход статического равномерного деления работы не всегда является оптимальным.

Однако более сложные алгоритмы распределения задач, адаптирующиеся под текущую нагрузку, ожидаемо имеют большие накладные расходы по сравнению с наивным подходом. Это, в частности, заметно при сравнениях современного планировщика задач OneTBB и статического подхода из OpenMP. Связано это с тем, что в основе планировщиков задач лежит подход work-stealing, позволяющий простаивающим потокам «воровать» работу у других в процессе выполнения алгоритма, но приводящий к конфликтам за общие ресурсы между потоками.

В то же время классический OpenMP при условии равномерной нагрузки имеет более высокую производительность за счёт меньших накладных расходов на распределение задач. Однако OpenMP практически не поддерживает вложенный параллелизм и балансировку нагрузки, что не позволяет его использовать в качестве универсального решения. 

Таким образом, существует потребность в алгоритме, который будет совмещать разные парадигмы распределения работы и иметь одинаково высокую производительность вне зависимости от паттерна нагрузки. Вместе с тем, чтобы алгоритм был универсально применяем, он должен поддерживать компонуемость -- задачи должны уметь запускать другие задачи, которые будут выполняться теми же потоками и тем самым порождать вложенный параллелизм.

Цель этой работы -- разработать алгоритм распределения задач поверх компонуемого планировщика задач, производительный вне зависимости от паттерна нагрузки, в том числе при наличии вложенного параллелизма в программе. Несмотря на то, что параллельных алгоритмов существует достаточно много, в работе будет рассматриваться распределение работы, порождаемой вызовом \texttt{parallel\_for}, как одного из основных примитивов параллелизма.

Для достижения этой цели в рамках работы были решены следующие задачи:
\begin{itemize}
  \item Разработка бенчмарков с различными паттернами нагрузки, использующих \texttt{parallel\_for}
  \item Сравнение современных решений (OneTBB и OpenMP) в различных конфигурациях
  \item Разработка прототипа алгоритма распределения задач, определение проблемные места и предложить их решения
  \item Сравнение получившегося алгоритма с известными реализациями параллельного исполнения на разработанных бенчмарках
\end{itemize}

В первой главе работы представлен обзор предметной области, анализ существующих подходов и постановка задачи. 

Во второй главе описаны реализованные бенчмарки и исследована разница в производительности между современными решениями. 

В третьей главе описан предлагаемый алгоритм, а также приведены результаты сравнений с наиболее популярными библиотеками для параллельного исполнения -- OpenMP и OneTBB.  

%% Начало содержательной части.
\chapter{Обзор предметной области}
В этой главе определены используемые в работе термины, подробно сформулирована решаемая задача, а также приведен обзор аналогов, решающих такую же или похожую задачу.

\section{Используемые термины и понятия}

В рамках модели параллельных вычислений мы будем оперировать понятиями потока и ядра процессора. Ядро процессора -- самостоятельный физический вычислительный блок процессора, выполняющий заданную последовательность инструкций. Современные процессоры, используемые в серверах, насчитывают порядка 128 ядер.
Поток же -- это логическая сущность внутри программы, имеющая свой стек, набор регистров и последовательность выполняемых инструкций. Планировщик операционной системы для каждого ядра процессора определяет, какой поток в данный момент времени будет на нём выполняться, переключая контекст выполнения между ними. 

В этой работе мы будем фокусироваться только на работе с потоками. На практике в программах, большая часть работы которых это выполнение параллельных алгоритмов, прибегают к привязке потоков к определённым ядрам. Например, на Linux это это достигается выставлением каждому потоку соответствующей битовой маски, задающей набор ядер процессора, на которых он может запускаться, с помощью системного вызова \texttt{sched\_setaffinity} ~\cite{sched-setaffinity}. Несмотря на то, что однозначное соответствие ядер потокам не даёт планировщику более оптимально распределять процессорное время между потоками разных процессов, это более чем оправданно, если на сервере параллельно не работают другие процессы, требовательные к вычислительным ресурсам. Такой механизм позволяет избежать миграции потоков между ядрами, которая приводит к дорогостоящей смене контекстов и может повлиять на стабильность замеров производительности ~\cite{protbb-pinning}.

Также регулярно упоминается пул потоков -- это механизм управления множеством потоков, позволяющий переиспользовать потоки для выполнения задач вместо пересоздания новых. Обычно пул потоков реализован это в виде простого интерфейса, приведённого в листинге ~\ref{lstpool}, поэтому альтернативно пул потоков можно называть и планировщиком задач -- он принимает задачу и планирует её выполнение на одном из потоков.
Задачей в свою очередь в простейшем случае можно просто называть функцию без аргументов, которую необходимо выполнить в любом из потоков.

\begin{algorithm}[!h]
\caption{Интерфейс пула потоков}\label{lstpool}
\begin{lstlisting}
class ThreadPool {
    ThreadPool(size_t threadNum);
    void Submit(F func);
};
\end{lstlisting}
\end{algorithm}

Обычно планировщик задач реализован в виде какой-то очереди, а созданные потоки в цикле вытаскивают из очереди задачу и выполняют её. Для уменьшения конфликтов потоков за общие ресурсы (англ. contention) используют несколько очередей -- например, по одной локальной очереди на поток. В таком случае, задача попадает в случайную из очередей, а потоки забирают задачу только из назначенной ему локальной очереди. Чтобы повысить утилизацию ресурсов потоков в ситуациях, когда у одного из потоков задачи кончились, а другого нет, применяется механизм захвата работы (англ. work-stealing) -- поток, у которого кончились задачи, итерируется по очередям других потоков и пробует забрать задачу оттуда.

В работе мы будем фокусироваться на распределении работы, порожденной параллельными алгоритами, поэтому стоит дать следующие определения. Параллельным алгоритмом будем называть алгоритм, исполнение которого разбивается на вычисление независимых подзадач, которое можно делать параллельно. Обобщённо такие подзадачи будем называть нагрузкой или работой, которую порождает этот алгоритм. Нагрузка может быть сбалансированной или несбалансированной -- нагрузка сбалансирована, если подзадачи, порождаемые алгоритмом, в среднем занимают одинаковое время на выполнение при идентичных условиях, в противном случае нагрузка несбалансированная.

Кроме того, следующие термины будут употребляться без перевода из-за отсутствия общепринятых аналогов:
\begin{itemize}
    \item  Oversubscription (дословно, переподписка) - ситуация, когда число активных потоков превышает количество доступных для их выполнения ядер в системе.
\end{itemize}


\section{Описание аналогов}
\startrelatedwork
На данный момент в индустрии распространены две библиотеки для параллельных вычислений на C++, которые имплементируют два различных подхода -- Intel OneTBB и OpenMP, с ними в этой работе и будут проводиться сравнения. Несмотря на то, что мы ограничиваемся библиотеками на одном языке (в том числе для удобства тестирования гипотез), фактически для высоко-производительных вычислений у C++ сейчас нет аналогов, поэтому рассматривать библиотеки и фреймворки на других языках излишне.

\subsection{OneTBB}
OneTBB -- это библиотека, предоставляющая интерфейсы различного уровня абстракций, от низкоуровневого интерфейса задач до параллельных алгоритмов, таких как \texttt{parallel\_for}, \texttt{parallel\_reduce} и другие. Подключается как обычная C++ библиотека, поэтому не требует какой-либо специфичной поддержки от компилятора. 

Стоит отметить, что весь дизайн OneTBB ориентирован на порождение и выполнении задач (Task-Based \cite{tbb-task-based}) -- благодаря этому достигается высокая производительность за счёт возможности балансировать нагрузку перемещением задач, а также появляется возможность оптимально реализовать различные сценарии композиции параллельности, такие как вложенность и конкуррентность \cite{protbb-composability}. Задачи, порождаемые программой, выполняются потоками из глобального пула потоков, который иницаилизируется на страте программы.
В такой модели параллельные алгоритмы, в частности \texttt{parallel\_for}, должны порождать задачи, распределять их по потокам и дожидаться завершения. Для \texttt{parallel\_for} это можно выразить совсем естественным образом -- каждая итерация представляется в виде одной задачи, вызывающей тело цикла на одном значении индекса. Однако очевидно, что в таком случае накладные расходы на создание и перемещение по памяти задачи могут оказаться выше, чем время на выполнение самого тела функции. Альтернативно, можно было бы разбить весь диапазон на число задач, равное числу потоков, но в таком случае при несбалансированной нагрузке некоторые задачи могли бы требовать в разы больше времени, чем другие, и вычислительные ресурсы потоков не были бы использованы наиболее оптимальным образом.

В OneTBB задачу оптимального разбиения диапазона работы на задачи решают так называемые разделители (англ. partitioner).
Кроме собственно разделения диапазона на меньшие диапазоны, они определяют стратегию распределения этих диапазонов по потокам.

OneTBB предоставляет следующие разделители \cite{tbb-partitioners}:
\begin{itemize}
    \item \texttt{auto\_partitioner} -- стратегия по умолчанию, изначально делит диапазон на большие поддиапазоны и по необходимости дробит дальше, основываясь на информации о том, была ли захвачена работа другим потоком через механизм work-stealing.
    \item \texttt{simple\_partitioner} -- делит диапазон, пока его размер не больше заданной заранее константы.
    \item \texttt{affinity\_partitioner} -- распределяет задачи на основе информации о распределении с предыдущей итерации, чтобы уменьшить процент кеш-промахов.
    \item \texttt{static\_partitioner} -- равномерное распределение итераций без балансировки нагрузки, имеет смысл использовать, если заранее известно, что нагрузка равномерно распределена.
\end{itemize}

\subsection{OpenMP}
OpenMP же, в отличие от OneTBB, требует поддержки компилятором, так как преобразовывает исходный код, основываясь на директивах, написанных в нём. В частности, для параллельного исполнения цикла \texttt{for} необходимо добавить директиву \texttt{\#pragma omp parallel for}, по необходимости определив стратегию планирования, которую нужно применять (см. листинг ~\ref{lstomp}).

OpenMP поддерживает следующие стратегии планирования (распределения работы) \cite{omp-spec}:
\begin{itemize}
    \item \textbf{static} -- статическое распределение работы диапазонами одинакового размера (по числу потоков).
    \item \textbf{dynamic} -- динамическое распределение работы через общую очередь диапазонов одинакового размера.
    \item \textbf{guided} -- аналогичен \textbf{dynamic}, но размеры диапазонов не одинаковы, а убывают пропорционально числу нераспределенных итераций.
\end{itemize}

Вдобавок стратегии \texttt{dynamic} и \texttt{guided} имеют модификатор \texttt{monotonic} (или \texttt{nonmonotonic}), специфицирующие порядок исполнения -- если установен модификатор \texttt{monotonic}, то каждый из потоков будет после итерации с индексом $i$ выполнять только итерации с большим номером.

\begin{algorithm}[!h]
\caption{Параллельный цикл \texttt{for} в OpenMP со стратегией \texttt{static}}\label{lstomp}
\begin{lstlisting}
#pragma omp parallel 
#pragma omp for schedule(static)
for (int i = from; i < to; ++i) {
    func(i);
}
\end{lstlisting}
\end{algorithm}

OpenMP для вычисления параллельных алгоритмов использует \textbf{fork-join} модель -- когда исполнение «доходит» до директивы \texttt{\#pragma omp parallel}, поток создаёт по необходимости (в зависимости от уровня параллелизма, который можно указать вызовом функции \texttt{omp\_set\_num\_threads} или через переменную окружения \texttt{OMP\_NUM\_THREADS}) дополнительные потоки, которые будут выполнять код параллельного региона. Если в регионе есть директива \texttt{\#pragma omp for} и цикл \texttt{for} после него, то диапазон такого цикла будет каким-либо образом разделен между потоками таким образом, что каждая итерация будет выполнена ровно один раз в каком-либо из потоков. В конце параллельно региона находится неявный с точки зрения пользовательского кода барьер.  С точки зрения реализации все эти директивы заменяются на этапе компиляции на сооответствующий код в промежуточном представлении компилятора (создание потоков, барьер и т.д.).

У OpenMP есть несколько реализаций, для сравнений в этой работе была выбрана реализация LLVM как наиболее активно развивающаяся по части улучшения производительности и используемая в аналогичных статьях и исследованиях. ~\cite{lbomp}

\subsection{Проблемы существующих решений}
Несмотря на то, что уже есть два популярных в индустрии решения, не решены следующие проблемы:
\begin{itemize}
    \item Статический подход к распределению работы, реализованный в OpenMP, показывает низкую производительность на несбалансированной нагрузке.
    \item Подход планировки задач с захватом работы (work-stealing), реализованный в OneTBB показывает низкую производительность при сбалансированной нагрузке за счёт накладных расходов на излишнюю балансировку.
    \item OpenMP фактически не поддерживает компонуемость и вложенный параллелизм, так как каждый вложенный вызов порождает новые потоки.
\end{itemize}

%% Пример ссылок в рамках обзора: \cite{example-english, example-russian, unrestricted-jump-evco, doerr-doerr-lambda-lambda-self-adjustment-arxiv}.

%% Так помечается конец обзора.
\finishrelatedwork
% Вне обзора:~\cite{bellman}.
\section{Постановка задачи}
Как можно понять из предыдущего раздела, существует потребность в универсальном решении, которое бы имело удовлетворительную производительность вне зависимости от характера нагрузки, так как он может зависеть от входных данных и не всегда можно выбрать оптимальный подход на этапе написания кода. Помимо этого, необходимо решить и проблему компонуемости -- универсальное решение должно поддерживать вложенный параллелизм без многократной деградации производительности, как это происходит в OpenMP из-за создания новых потоков.

Понятно, что задача оптимального распределения работы для параллельного алгоритма специфична для конкретного алгоритма, поэтому в работе рассматривается преимущественно распределение работы для \texttt{parallel\_for}, как наиболее часто используемого примитива, в том числе позволяющего строить поверх него другие, более сложные алгоритмы.

Реализовывать с нуля пул потоков с захватом работы нецелесообразно, учитывая наличие множества оптимально работающих решений и то, что это не является целью работы. Вместо этого, был использован готовый пул потоков из библиотеки Eigen, разработанный Дмитрием Вьюковым, который также проектировал планировщик для языка Go.

Необходимо реализовать поверх выбранного планировщика задач оптимальный алгоритм распределения работы, порождаемой \texttt{parallel\_for}, по необходимости внося изменения в сам планировщик.

\chapterconclusion 
В этой главе был приведен обзор предметной области выполнения параллельных алгоритмов, в частности объяснены используемые далее термины и понятия, а также приведено описание двух популярных в индустрии подходам к выполнению параллельных алгоритмов. Также была сформулирована задача, которая решается в этой работе.

\chapter{Бенчмарки и анализ производительности аналогов}
В этой главе представлены бенчмарки, созданные для оценки производительности разных реализаций алгоритма \texttt{parallel\_for} на различных типах нагрузки, а также приведено описание систем, результаты запуска на которых используются в работе. Помимо того, проведен анализ производительности существующих популярных решений, рассмотренных в предыдущей главе.

\section{Бенчмарки}

TODO: spmv triangle, hyper
\begin{table}
\caption{Обзор характеристик бенчмарков}
\begin{center}
\begin{tabular}{cccc}
\hline
\multicolumn{1}{c}{\rule{0pt}{10pt}
                   Название}&\multicolumn{1}{c}{Нагрузка}&\multicolumn{1}{c}{Вызовов \texttt{parallel\_for} и итераций}\\[2pt]
\hline\rule{0pt}{12pt}
SPMV-balanced & Сбалансирована & 1 по $N$ \\
SPMV-unbalanced & Несбалансирована & 1 по $N$ \\
Scan & Сбалансирована & $2\log{N}$ по $\frac{N}{2^i}, i = 1..\log{N}$ \\
Reduce & Сбалансирована & 1 по $\frac{N}{1024}$ \\
Scheduling time & Сбалансирована & 1 по \texttt{num\_threads} \\
MatrixTranspose & Сбалансирована &  \begin{tabular}{@{}c@{}} Вложенный: 1 вызов из 16 итераций, \\ в каждой ещё вызов из 16 итераций\end{tabular} \\
MatrixMultiply & Сбалансирована & \begin{tabular}{@{}c@{}} Вложенный: 1 вызов из $Rows$ итераций, \\ в каждой ещё 1 вызов из $Cols$ итераций \end{tabular} \\[2pt]
\hline
\end{tabular}
\end{center}
\label{tab:benchmarks-overview}
\end{table}

\subsection{Умножение разреженной матрицы на вектор (SPMV)}
Умножение матрицы на вектор -- это часто используемая операция в высокопроизводительных вычислениях, особенно для разреженных матриц, что часто встречается, например, в машинном обучении. В этом бенчмарке мы используем сжатое представление матрицы в формате Compressed Sparse Row (CSR), где каждая матрица представлена тремя массивами: ненулевыми значениями, индексами их столбцов и массивом из указателей в массив индексов столбцов по каждой строке. Умножение матрицы и вектора производится путем итерации по строкам матрицы и умножения каждой строки на вектор. Эта задача хорошо параллелизуется, так как каждую строку можно умножать независимо. Код параллелизации такого умножения представлен в листинге ~\ref{lstspmv}.

\begin{algorithm}[!h]
\caption{Параллельное умножение разреженной матрицы на вектор}\label{lstspmv}
\begin{lstlisting}
double MultiplyRow(SparseMatrix m, Vector column, size_t row) {
  double res = 0;
  for (size_t i = m.RowIndex[row]; i < m.RowIndex[row + 1]; ++i) {
    res += m.Values[i] * column[m.ColumnIndex[i]];
  }
  return res;
}

Vector MultiplyMatrix(SparseMatrix m, Vector column) {
  Vector out(m.Rows);
  ParallelFor(0, m.Rows, [&](size_t i) {
    out[i] = MultiplyRow(m, column, i);
  });
  return out;
}
\end{lstlisting}
\end{algorithm}

Мы будем фокусироваться на двух интересных случаях: сбалансированных и несбалансированных матрицах. В контексте этой задачи матрица считается сбалансированной, если количество ненулевых элементов в каждой строке примерно одинаково -- размер всех задач будет примерно одинаковым. В противном случае мы будем называть матрицу несбалансированной.

\subsubsection{Сбалансированная версия}
В этом бенчмарке были сгенерированы сбалансированные матрицы фиксированного размера, в которых количество элементов равномерно распределено между строками и столбцами. Ожидается, что на этом бенчмарке будут наиболее эффективны подходы статического распределения работы, так как он более предсказуем и может быть легко распараллелен путем равномерного деления между потоками.

\subsubsection{Несбалансированная версия}
Несбалансированная матрица может быть несбалансирована по-разному: с одинаковым количеством элементов размер задач может распределяться по-разному. Мы будем сравнивать два случая: с равномерно убывающим количеством элементов в строке (назовем его \textbf{spmv\_triangle} бенчмарк) и с гиперболически убывающим количеством элементов (\textbf{spmv\_hyperbolic} бенчмарк).

От динамического подхода распределения ожидается большая эффективность для несбалансированных матриц. При равномерном статическом планировании некоторые потоки закончат работу раньше и будут простаивать, пока другие потоки все еще работают. Динамическое распределение позволит лучше балансировать нагрузку между потоками, но оно более вычислительно затратное из-за накладных расходов на кражу работы или другие виды синхронизации.

\subsection{Свёртка (reduce)}
Этот бенчмарк вычисляет сумму массива целых чисел, разбивая его на блоки заранее заданного размера и считая сумму в каждом блоке параллельно. Этот бенчмарк представляет случай полностью сбалансированной нагрузки, но по сравнению с бенчмарком умножения разреженной матрицы на столбец размер каждой порции данных в этом бенчмарке больше. 
Вследствие чего мы ожидаем низкие накладные расходы относительно времени выполнения самих задач и высокую производительность как для статического алгоритма планирования, так и для динамического.

\subsection{Сканирование (scan)}
Scan -- это параллельный алгоритм, который вычисляет все префиксные суммы массива целых чисел.
Мы используем реализацию с двумя фазами: Up-Sweep (Reduce) и Down-Sweep, как описано в статье «Parallel Prefix Sum (Scan) with CUDA» от NVIDIA~\cite{nvidia-scan}.
Этот бенчмарк интересен тем, что он содержит множество ($2\log {N}$) вызовов \texttt{parallel\_for}, часть из которых выполняет небольшое число лёгких задач, что позволяет пронаблюдать накладные расходы на фазы инициализации и завершения алгоритма.

\subsection{Вложенный параллелизм}
Для проверки производительности решений в условиях применения вложенного параллелизма было реализовано два похожих бенчмарка: \texttt{MMultiple} (умножение двух матрицы) и \texttt{MTranspose} (транспонирование матрицы).
Отличаются эти бенчмарки тем, что умножение двух матриц имеет большие диапазоны (внешний \texttt{parallel\_for} итерируется по строкам матрицы, а внутренний -- по столбцам), когда транспонирование матриц делит матрицу на фиксированное число блоков и итерируется только по ним. 
Псевдокод этих бенчмарков представлен на листингах ~\ref{lstmmul} и ~\ref{lstmtranspose} соответственно.

\begin{algorithm}[!h]
\caption{Параллельное умножение двух матриц}\label{lstmmul}
\begin{lstlisting}
DenseMatrix MultiplyMatrix(DenseMatrix A, DenseMatrix B) {
    DenseMatrix out(A.Rows, B.Columns);
    ParallelFor(0, out.Rows, [&](size_t row) {
        ParallelFor(0, out.Columns, [&](size_t col) {
            double sum = 0;
            for (size_t j = 0; j != A.Columns; ++j) {
                sum += A[row][j] * B[j][col];
            }
            out[row][col] = sum;
        });
    });
}
\end{lstlisting}
\end{algorithm}

\begin{algorithm}[!h]
\caption{Параллельное транспонирование матрицы}\label{lstmtranspose}
\begin{lstlisting}
static constexpr size_t BLOCKS = 16;
void TransposeMatrix(DenseMatrix &input, DenseMatrix &out) {
    ParallelFor(0, BLOCKS, [&](size_t row) {
        ParallelFor(0, BLOCKS, [&](size_t column) {
            // calc fromRow, toRow, etc...
            for (size_t i = fromRow; i != toRow; ++i) {
                for (size_t j = fromCol; j != toCol; ++j) {
                    out[j][i] = input[i][j];
                }
            }
        });
    });
}
\end{lstlisting}
\end{algorithm}


\subsection{Время распределения задач}
Также был разработан специальный бенчмарк (будем называть его \textbf{scheduling\_time}) для наблюдения за скоростью распределения задач между потоками. Основное внимание в этом бенчмарке уделяется измерению времени между моментом вызова \texttt{parallel\_for} и моментами, когда каждый поток начинает выполнять свою первую задачу. Результаты этого бенчмарка помогают понять, сколько времени планировщик тратит на распределение задач. Ожидается, что в этом случае статический подход к планированию будет более эффективен, поскольку не требуется никакой синхронизации между потоками.

\section{Конфигурация системы}
Все бенчмарки были запущены на двух системах: на виртуальной машине c 48 ядрами в облачном окружении с двумя процессорами Intel Xeon Gold 6338 (c отключенным HyperThreading) и на 48-ядерном Huawei Kunpeng 920 на архитектуре ARM.

Вычислительно интенсивные бенчмарки были выполнены с использованием библиотеки Google Benchmark. Бенчмарк времени распределения задач планировщиком выполнялся с помощью собственной реализации сбора данных, а время выполнения было измерено в тактах процессора с использованием инструкции rdtsc для x86~\cite{rdtsc} и инструкции \texttt{mrs \%0, cntvct\_el0} для ARM ~\cite{rdtsc-arm}.

Для каждого бенчмарка были протестированы различные конфигурации библиотек. Реализация OpenMP использовалась из библиотеки LLVM OpenMP v16.0.2 с различными политиками планирования: static, dynamic и guided. Политики планирования guided и dynamic были проверены в комбинации с параметром nonmonotonic и monotonic.
Потоки привязывались к ядрам, используя переменную среды \texttt{OMP\_PROC\_BIND=close}, чтобы избежать миграции одного потока на другое ядро, отличное от того, на котором он изначально был запущен.

Кроме того, были установлены значения \texttt{OMP\_WAIT\_POLICY=active} и \texttt{KMP\_BLOCKTIME=infinite}, чтобы гарантировать, что потоки не «засыпают» во время ожидания задач, что могло бы привести к нестабильным результатам. Для включения вложенного параллелизма использовалось выставление параметр \texttt{OMP\_MAX\_ACTIVE\_LEVELS=8}, по умолчанию в OpenMP вложенный параллелизм отключен как раз по причине его низкой производительности -- так как он создает новые потоки, это часто приводить к тому, что потоков становится больше чем ядер, в результате чего стремительно деградирует производительность из-за их переключений. Помимо этого, в принципе операция старта новых потоков занимает существенное время.

Для oneTBB использовался планировщик по умолчанию с настройками по умолчанию, все потоки были привязаны к ядрам с использованием observer. ~\cite{protbb-pinning}

Чтобы убедиться, что при измерении производительности будут задействованы все потоки, перед запуском измеряемой части бенчмарков проводился прогрев блокирующими задачами, активно ожидающими, пока начнется выполнение на всех потоках. Такая процедура помогает бороться с тем, что библиотеки (и TBB, и OpenMP) создают потоки «лениво», что могло бы сказаться на воспроизводимости результатов.

\section{Результаты OpenMP и TBB на бенчмарках}
В данном разделе представлены результаты бенчмарков и обсуждаются причины наблюдаемого поведения. Основной фокус направлен на сравнение производительности статического и динамического подходов к планированию, чтобы понять преимущества и недостатки каждого из подходов.

\subsection{SPMV}
Результаты бенчмарка SPMV представлены на рисунках~\ref{Fig:SpmvBalanced}, ~\ref{Fig:SpmvTriangle} и ~\ref{Fig:SpmvHyper}. Для демонстрации зависимости производительности от размера задач бенчмарк запускался с разным параметром \texttt{width}, отвечающим за размер строки -- от неё прямо пропорционально число действий на каждой итерации цикла. По оси X отмечено значение параметра, а по оси Y -- время на выполнение бенчмарка в микросекундах по логарифмической шкале. Заметно, что с увеличением размера задач сокращается разрыв в производительности между различными алгоритмами, что в целом объяснимо тем, что на задачах большего размера накладные расходы на распределение задач менее заметны на фоне времени, затраченного на выполнение тела задач -- особенно это заметно при сбалансированной нагрузке.


\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/bench_spmv_balanced.svg}
\caption{Бенчмарк SPMV Balanced с разным параметром длины строк}\label{Fig:SpmvBalanced}
\end{figure}

\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/bench_spmv_triangle.svg}
\caption{Бенчмарк SPMV Triangle с разным параметром длины строк}\label{Fig:SpmvTriangle}
\end{figure}

\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/bench_spmv_hyperbolic.svg}
\caption{Бенчмарк SPMV Hyperbolic с разным параметром длины строк}\label{Fig:SpmvHyper}
\end{figure}


\subsubsection{Сбалансированная матрица}
Как ожидалось, в данном случае статический подход к планированию оказался более эффективным, чем динамический. Это особенно заметно при сравнении различных политик планирования одной и такой же библиотеки: статическое распределение работы в OpenMP оказалось более эффективным, чем динамическое -- среднее время выполнения на 25\% ниже. Результаты TBB показали, что \texttt{affinity\_parititoner} в данном случае лидирует, среднее время выполнения у него на 5\% ниже, чем у \texttt{auto\_partitioner}. Эта разница в производительности может быть связана с равномерным распределением итераций и лучшей кэш-аффинити, используемой \texttt{affinity\_parititoner}, как указано в документации oneTBB \cite{tbb-partitioners}.

\subsubsection{Несбалансированная матрица}
В отличие от сценария сбалансированной нагрузки, в этом случае политика динамического планирования оказалась более эффективной по сравнению со статической политикой.
Наибольшую эффективность продемонстрировала динамическая политика OpenMP с модификатором nonmonotonic -- среднее время выполнения оказалось в 1,25 раза ниже, чем у TBB с простым partitioner'ом и в 10 раз ниже, чем у статической политики OpenMP.
Также можно заметить, что в случае гиперболически изменяющегося распределения элементов матрицы разница в производительности между динамической и статической стратегиями планирования более заметна, чем в случае треугольной матрицы -- это объясняется тем, что в треугольной матрице для задач меньшего размера накладные расходы куда больше влияют на время выполнение бенчмарка, из-за чего даже на такой несбалансированной нагрузке OMP Static может показывать удовлетворительный результат.

\subsection{Reduce}
Результаты бенчмарка Reduce представлены на рисунке~\ref{Fig:Scan}. Результаты ожидаемо похожи на сбалансированную версию бенчмарка \textbf{SPMV} -- лучшие результаты у OMP Static и TBB Affinity, что ожидаемо для сбалансированной нагрузки.

\subsection{Scan}
Результаты бенчмарка Scan представлены на рисунке~\ref{Fig:Scan}.
Политика динамического планирования в OpenMP показала низкую эффективность, с более медленным временем выполнения по сравнению с последовательной версией алгоритма.
Это может быть связано с накладными расходами на начальное распределение задач и кражу работы при малых размерах задач.
Статическая политика OpenMP оказалась наиболее эффективной, демонстрируя почти в 2 раза быстрое выполнение по сравнению с другими политиками.


\begin{figure}[!htb]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_scan.svg}
    \caption{Scan}\label{Fig:Scan}
\end{figure}

\begin{figure}[!htb]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_reduce.svg}
    \caption{Reduce}\label{Fig:Reduce}
\end{figure}

\subsection{Вложенный параллелизм}
По результатам этих бенчмарков, представленных на рисунках~\ref{Fig:Mmul} и ~\ref{Fig:Mtranspose}, особенно заметно, что OpenMP не поддерживает вложенный параллелизм -- производительность деградирует по сравнению с TBB на порядок, что объясняется как раз тем, что OpenMP создаёт новые потоки на каждый вложенный вызов. При этом разрыв в транспонировании матриц в разы сильнее из-за  меньшего размера задач, на фоне которого больше выделяются накладные расходы на создание потоков при вложенности.
\begin{figure}[!htb]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_mmul.svg}
    \caption{MMultiply}\label{Fig:Mmul}
\end{figure}

\begin{figure}[!htb]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_mtranspose.svg}
    \caption{MTranspose}\label{Fig:Mtranspose}
\end{figure}
    
\subsection{Время распределения задач}
В этом бенчмарке сравнивается время, необходимое планировщику для распределения работы между потоками.
Результаты представлены на рисунке~\ref{Fig:SchedulingTime}.
На оси X показаны номера потоков (отсортированных по времени выполнения первой задачи), а на оси Y -- время, затраченно на планирование задачи (в циклах).

Результаты показали, что стратегия статического планирования в OpenMP является наиболее эффективной. Это ожидаемый результат, так как статическое планирование требует только деления работы между потоками, в то время как динамическое планирование требует дополнительной синхронизации потоков при работе с очередью задач. Кроме того, среднее время планирования оставалось примерно равным для всех потоков.

Почти все разделители из TBB имели похожую друг на друга производительность, при этом \texttt{affinity\_parittioner} оказался наиболее эффективным. Однако время распределения задач между потоками в TBB неодинаково для всех потоков. В частности, последние потоки имели значительно более высокое время планирования, чем первые -- это может происходить из-за накладных расходов на захват работы, так как последним потокам нужно проверять больше очередей других потоков при захвате задач.

\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/scheduling_time.svg}
\caption{Время от вызова \texttt{parallel\_for} до начала выполнения первой задачи}\label{Fig:SchedulingTime}
\end{figure}

\chapterconclusion
В этой главе было приведено описание разработанных бенчмарков, а также результаты проверки на этих бенчмарков популярных аналогов решаемой задачи. 

В целом, различные конфигурации OneTBB и OpenMP показывают ожидаемые результаты на разных по характеристикам бенчмарках. Так, статический вариант из OpenMP (\texttt{OMP\_STATIC}) имеет наилучшую производительность на бенчмарках с равномерно распределенной нагрузкой (\texttt{spmv\_balanced}, \texttt{reduce}) и на бенчмарке, требовательном к времени старта и завершения задач (\texttt{scan}), но при этом отсутсвие балансировки нагрузки проявляется на бенчмарках с несбалансированной нагрузкой. Динамический подход к распределению задач что в TBB, что в OpenMP показывает неплохие результаты на несбалансированной нагрузке и ожидаемо проигрывает статическому подходу на сбалансированной, при этом реализация из OpenMP заметно уступает OneTBB.

Как итог, ни одно из решений нельзя назвать универсальным TODO: почему? как-то красиво сказать что в непредссказуемой нагрузке любой из подходов будет иметь минусы т.к. парадигмы разные

\chapter{Предлагаемое решение}
В этой главе описан алгоритм предлагаемого решения, а также разобраны проблемные места в распределении задач, которые были решены. Помимо того, приведены результаты бенчмарков, сравнивающие получившуюся реализацию с аналогами (OpenMP и OneTBB).

\section{Узкие места и их решения}
Как можно понять из предыдущих глав, для реализации оптимального компонуемого \texttt{parallel\_for} нужно решить следующие проблемы:
\begin{enumerate}
    \item Каким-либо образом распределить изначально работу по потокам.
    \item Во время исполнения по необходимости перераспределять работу между потоками, если инзачальное распределение оказалось несбалансированным.
\end{enumerate}

На примере OneTBB можно убедиться, что удобно эти проблемы решать, декомпозировав алгоритм на две составляющих -- разделитель работы (partitioner) и планировщик задач с механизмом захваты работы (work-stealing пул потоков). В таком случае мы получаем производительную компонуемость -- любой вложенный или конкурентный параллелизм будет работать на том же пуле потоков, не приводя к oversubscription и связанным с ним негативным эффектам в виде падения производительности. Другими словами, мы переносим задачу планирования нагрузки из слоя операционной системы (планирование выполнения потоков между ядрами) в пользовательский код и переходим к планированию задач между потоками, чем снижаем накладные расходы за счёт меньшего переключения контекста потоков и переходов в пространство ядра.

Однако самого себе распределения задач посредством work-stealing в пуле потоков не достаточно -- как мы видим на примере \texttt{simple\_partitioner} из TBB, такой подход оказывается не самым оптимальным из-за описанных далее недостатков.


Во-первых, распределение задач по потокам с помощью work-stealing плохо масштабируется -- если потоки вынуждены искать задачи в очередях других потоков, время на поиск потоком задач растёт по мере уменьшения числа задач, так как потокам приходится проверить большее число очередй. Это особо заметно на небольшом числе задач, что видно по результатам бенчмарка с распределением времени старта выполнения задач, представленном на рисунке~\ref{Fig:SchedulingTime}.

Во-вторых, заранее невозможно выбрать универсальное разделение диапазона итераций на задачи (в терминах планировщика) оптимального размера, так как невозможно предугадать время исполнения каждой итерации. Реализации в TBB (Simple partitioner) и OpenMP (Dynamic) по сути не решают эту проблему и отдают её пользователю, предоставляя возможность указать минимальный размер диапазона (по умолчанию равный 1), что не всегда подходит, если заранее не известны параметры нагрузки. Низкое значение этого параметра при небольшом размере задач приводит к тому, что накладные расходы начинают привышать затраты на выполнение непосредственно задач, а слишком большое значение не даёт оптимально балансировать нагрузку.

Альтернативно изначальному распределению задач через механизм случайного work-stealing предлагается использовать явное распределение задач по потокам (work-sharing) -- будем явно и детерминированно распределять задачи по очередям потоков, не полагаясь на work-stealing. При этом очевидно, что если распределять будет один поток, то время распределения зависит линейно от числа потоков в системе -- например, такая проблема наблюдается у \texttt{simple\_partitioner} и \texttt{auto\_partitioner} в TBB. Здесь предлагается следующая оптимизацию: распределять изначально задачи по потокам в виде $K$-ичного дерева. В зависимости от числа ядер в системе значение параметра $K$ может варьироваться, в простейшем случае можно использовать $K = 2$ -- по результатам бенчмарка на рисунке~\ref{Fig:SchedulingTimeTree} заметна разница между линейным распределением и деревьями с разным количество детей. Распределение будет работать следующим образом:
\begin{enumerate}[\hspace{18pt}1)]
    \item Поток с номером $L$ задачу с диапазоном итераций $[l, r)$ и диапазоном потоков $[L, R)$, задачи по которым ещё не были распределены.
    \item Если $R - L \neq 1$, то из диапазона $[L+1, R)$ выбираются $K$ потоков и на них запускается та же задача с равномерно разбитым по ним диапазоном итераций $[l + \frac{r-l}{R - L}, r)$.
    \item Остальные $[l, l + \frac{r-l}{R - L})$ итераций выполняются в этом потоке, по мере их выполнения создаются балансировочные задачи и добавляются в очередь этого потока.
\end{enumerate}

% \begin{algorithm}[!h]
% \caption{Пример псевдокода}\label{lst2}
% \begin{algorithmic}
% 	\Function{IsPrime}{$N$}
% 		\For{$t \gets [2; \lfloor\sqrt{N}\rfloor]$}
% 			\If{$N \bmod t = 0$}
% 				\State\Return \textsc{false}
% 			\EndIf
% 		\EndFor
% 		\State\Return \textsc{true}
% 	\EndFunction
% \end{algorithmic}
% \end{algorithm}



\begin{figure}[ht!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/scheduling_time_tree.svg}
\caption{Зависимость времени распределения задач от количества детей в дереве}\label{Fig:SchedulingTimeTree}
\end{figure}

TODO: пример рисунком

Учитывая, что проектируемое решение должно работать с компонуемым планировщиком задач, распределение работы нужно делать в виде одной из задач, которую планировщик принимает и выполняет. Для интеграции этого алгоритма с планировщиком задач, взятым из библиотеки Eigen, потребовались некоторые доработки.

Во-первых, при вызове \texttt{parallel\_for} вызывающий поток обязан дождаться завершения работы, но делать это ожидая на каком-нибудь барьере не эффективно -- поток может выполнять часть задач, как это происходит в OpenMP и TBB. Для этого в планировщике был реализован метод \texttt{Join()}, позволяющий вшешнему потоку, то есть не созданному планировщиком, подключиться к выполнению задач.
TODO:  возможно побольше расписать?

Во-вторых, для оптимальной работы недостаточно простого метода \texttt{Submit(F Func)} у планировщика, потому что в таком случае нет контроля над тем, на каких потоках предпочтительно будут исполнены задачи -- для поддержки такого в планировщик был добавлен метод \texttt{Submit(F func, ThreadId idx)}, добавляющий задачу в очередь указанного потока, а не случайного.

В результате получается дерево глубины $\log {N}$, где $N$ -- число задач, то есть время распределения задач растёт логарифмически с увеличением числа потоков. При этом разумно изначально делить задачи оптимистично на равные диапазоны по потокам -- в таком случае при сбалансированной нагрузке мы получим идеальное распределение.

TODO: а как дальше балансировочные таски создаём? почему так?

Однако в этом месте возникает конфликт парадигм распределения работы через её явную раздачу (work-sharing) и через механизм захвата работы (work-stealing). Он проявляется следующим образом: поток может захватить балансировочные задачи из другого потока до того, как к нему придёт его основной диапазон работы. Произойдёт преждевременный захват работы, который приводит к несбалансированному изначальному распределению нагрузки даже в случае идеально сбалансированных входных данных, а также к нетедерминированному распределению, что плохо сказывается на взаимодействиях с памятью и кешами.
TODO:  картинки

Решать эту проблему предлагается следующим образом: после того, как задача из изначального распределения начинает выполняться потоком, он не должен (TODO: выше написать про создание балансировочных задач) создавать балансировочные задачи до того момента, пока изначальное распределение работы не завершится, то есть пока все потоки не получат через алгоритм, описанный выше, свои задачи. Это можно было бы сделать через какую-нибудь общую синхронизацию, но понятно, что такое плохо масштабируется из-за конфликтов потоков на общей памяти (contention).

Альтернативный подход, который и был выбран -- вероятностный, основанный на времени, за которое на конкретной конфигурации на выбранном числе потоков задачи успевают распределиться по всем потокам. Сводится он к тому, что поток не сразу создаёт балансировочные задачи, а только спустя заранее заданный промежуток времени.

Понятно, что такой подход не гарантирует, что за этот промежуток времени задачи успеют распределиться по всем потокам, но вероятность этого можно увеличить правильным подбором константы. Для этого реализована отдельная программа (\texttt{timespan\_tuner}), запускающая большое число итераций ($10000$), состоящих из запуска \texttt{parallel\_for} из \texttt{num\_threads} задач, каждая из которых блокируется в ожидании запуска оставшихся. По каждой итерации считается статистика времени, которое ушло на распределение задач, а потом по всем итерациям берётся 99-я перцентиль -- при выборе такой константы ожидается, что в $99\%$ запусков распределение работы будет укладываться в этот промежуток времени. Время замеряется инструкцией rdtsc на x86~\cite{rdtsc} (и её альтернативой \texttt{mrs \%0, cntvct\_el0} на ARM ~\cite{rdtsc-arm}).

\section{Результаты бенчмарков решения}

На всех графиках, представленные ниже, полученное решение имеет имя \texttt{TIMESPAN\_GRAINSIZE}.

Результаты бенчмарка SPMV представлены на рисунках~\ref{Fig:SpmvBalancedFinal}, ~\ref{Fig:SpmvTriangleFinal} и ~\ref{Fig:SpmvHyperFinal}. Видно, что на сбалансированной нагрузке \texttt{TIMESPAN\_GRAINSIZE} уступает только статическому варианту OpenMP, а на несбалансированной уступает статическому на задачах небольшого размера (как и все аналоги с динамическим распределением работы), но на задачах большего размера показывает результаты лучше чем OpenMP static и аналоги из TBB. 

\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/bench_spmv_balanced_final.svg}
\caption{Бенчмарк SPMV Balanced с разным параметром длины строк}\label{Fig:SpmvBalancedFinal}
\end{figure}

\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/bench_spmv_triangle_final.svg}
\caption{Бенчмарк SPMV Triangle с разным параметром длины строк}\label{Fig:SpmvTriangleFinal}
\end{figure}

\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/bench_spmv_hyperbolic_final.svg}
\caption{Бенчмарк SPMV Hyperbolic с разным параметром длины строк}\label{Fig:SpmvHyperFinal}
\end{figure}

На рисунке~\ref{Fig:ReduceFinal} видны результаты алгоритма на бенчмарке Reduce, аналогично сбалансированной вариации бенчмарка SPMV полученный алгоритм проигрывает OpenMP static, но обходит все остальные аналоги.

\begin{figure}[!t]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_reduce_final.svg}
    \caption{Reduce}\label{Fig:ReduceFinal}
\end{figure}

На результате бенчмарка Scan (рисунок~\ref{Fig:ScanFinal}) также видно, что решение в разы обгоняет все аналоги, кроме OpenMP static -- объяснимо это тем, что накладные расходы на распределение работы у реализованного алгоритма всё же больше, чем у статического деления на блоки разного размера, однако они оказались ниже, чем у аналогов. Подтверждение этому видно на бенчмарке времени распределения работы (рисунок~\ref{Fig:SchedulingTimeFinal}) -- полученный алгоритм распределяет работу быстрее, чем алгоритмы из OneTBB, но медленее реализации из OpenMP.

\begin{figure}[!t]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_scan_final.svg}
    \caption{Scan}\label{Fig:ScanFinal}
\end{figure}

\begin{figure}[t!]
%\vspace{2.5cm}
\centering
\includesvg[width=\textwidth]{benchmarks/scheduling_time_final.svg}
\caption{Время от вызова \texttt{parallel\_for} до начала выполнения первой задачи}\label{Fig:SchedulingTimeFinal}
\end{figure}

Кроме того, реализованный алгоритм сохраняет свойство компонуемости -- на бенчмарках с вложенным параллелизмом, в отличие от OpenMP, алгоритм показывает удовлетворительную производительность, хоть и немногим уступает OneTBB (рисунки~\ref{Fig:MmulFinal} и ~\ref{Fig:MtransposeFinal}).

\begin{figure}[!htb]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_mmul_final.svg}
    \caption{MMultiply}\label{Fig:MmulFinal}
\end{figure}

\begin{figure}[!htb]
    \centering
        \includesvg[width=\linewidth]{benchmarks/bench_mtranspose_final.svg}
    \caption{MTranspose}\label{Fig:MtransposeFinal}
\end{figure}

\chapterconclusion

TODO

%% Макрос для заключения. Совместим со старым стилевиком.
\startconclusionpage

В данном разделе размещается заключение.

\printmainbibliography

%% После этой команды chapter будет генерировать приложения, нумерованные русскими буквами.
%% \startappendices из старого стилевика будет делать то же самое
\appendix

\end{document}
